# Balanced training config for 44GB GPU targeting ~60-70% GPU utilization
# Optimized for 4x GPUs with 44GB memory each
# Increased micro_batch_size from 1 to 2 for better throughput while staying under memory limit

model:
  d_model: 512       # Ultra small model for 44GB GPU
  n_layers: 8        # Reduced from 12
  n_heads: 8
  n_kv_heads: 2
  d_ffn: 1536        # Reduced from 2048
  vocab_size: 151936 # Qwen2.5 vocab size
  max_position_embeddings: 8192

  # GSA specific configuration
  gsa:
    d_indexer: 64
    n_indexer_heads: 4
    k_base: 512      # Reduced from 1024 for memory
    k_min: 64        # Reduced
    k_max: 1024      # Reduced
    use_value_gate: true
    use_output_gate: true
    use_adaptive_k: true

training:
  # Optimizer
  optimizer: adamw
  learning_rate: 3.0e-4
  min_learning_rate: 3.0e-5
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  eps: 1.0e-8

  # Schedule
  warmup_steps: 2000
  lr_scheduler: cosine
  max_steps: 100000

  # Batch size (balanced for 44GB GPU - targets ~60-70% usage)
  micro_batch_size: 2  # Increased from 1 to 2 for better GPU utilization
  gradient_accumulation_steps: 32  # Adjusted to maintain global batch size
  global_batch_size: 256  # 2 * 32 * 4 = 256

  # Sequence length (MUST match packed data)
  max_seq_len: 1024   # Keep at 1024 to avoid OOM

  # Precision
  precision: bf16
  gradient_checkpointing: true

  # Stability
  grad_clip: 1.0

  # GSA specific
  indexer_warmup_steps: 1000
  indexer_lr_multiplier: 10.0

data:
  # Use local pre-tokenized and packed data
  local_dataset_path: /data/hf_datasets/SlimPajama-627B  # Local copy of SlimPajama
  data_dir: data/packed  # Use pre-tokenized and packed data (4096 token sequences)
  dataset: cerebras/SlimPajama-627B  # Fallback if local_dataset_path and data_dir not found
  tokenizer: Qwen/Qwen2.5-7B
  num_workers: 4
  use_packing: true

logging:
  project: gsa-pretrain
  log_interval: 10
  eval_interval: 1000
  save_interval: 5000

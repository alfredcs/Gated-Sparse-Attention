# Ultra memory-efficient training config for 44GB GPU
# Optimized for 4x GPUs with limited memory

model:
  d_model: 768       # Smaller model (512M params)
  n_layers: 12       # Fewer layers
  n_heads: 8
  n_kv_heads: 2
  d_ffn: 2048        # Smaller FFN
  vocab_size: 151936 # Qwen2.5 vocab size
  max_position_embeddings: 8192

  # GSA specific configuration
  gsa:
    d_indexer: 64
    n_indexer_heads: 4
    k_base: 512      # Reduced from 1024 for memory
    k_min: 64        # Reduced
    k_max: 1024      # Reduced
    use_value_gate: true
    use_output_gate: true
    use_adaptive_k: true

training:
  # Optimizer
  optimizer: adamw
  learning_rate: 3.0e-4
  min_learning_rate: 3.0e-5
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  eps: 1.0e-8

  # Schedule
  warmup_steps: 2000
  lr_scheduler: cosine
  max_steps: 100000

  # Batch size (ultra memory-efficient)
  micro_batch_size: 1
  gradient_accumulation_steps: 32
  global_batch_size: 128  # 1 * 32 * 4 = 128

  # Sequence length (MUST match packed data)
  max_seq_len: 2048   # Shorter sequences for memory

  # Precision
  precision: bf16
  gradient_checkpointing: true

  # Stability
  grad_clip: 1.0

  # GSA specific
  indexer_warmup_steps: 1000
  indexer_lr_multiplier: 10.0

data:
  # Use local pre-tokenized and packed data
  local_dataset_path: /data/hf_datasets/SlimPajama-627B  # Local copy of SlimPajama
  data_dir: data/packed_2k  # Optional: Use pre-tokenized and packed data if available
  dataset: cerebras/SlimPajama-627B  # Fallback if local_dataset_path and data_dir not found
  tokenizer: Qwen/Qwen2.5-7B
  num_workers: 4
  use_packing: true

logging:
  project: gsa-pretrain
  log_interval: 10
  eval_interval: 1000
  save_interval: 5000
